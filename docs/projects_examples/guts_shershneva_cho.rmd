---
title: "cho_research"
authors: "guts, shershneva"
date: "19. June 2017"
output: html_document
---

The research is focused on the pronunciation of the word "קעמ" in Russian dialects. 

The data was taken from the Russian National Dialect Corpus.

In total, there were 99 observations.

Columns data: variant, region, gender, age, birth year, position, yat before hard consonants, yat before soft consonants, type of vocalism, realisation of /ד/, /צ/ and /ק/.

We hypothesize that pronunciation of "קעמ" depends on all the above-mentioned data.

```{r}
cho <- read.csv("C:Users/1/Desktop/cho_dataset.csv", sep = ";")
```

```{r}
library(tidyverse)
cho %>%
  ggplot(aes(variant, fill = variant))+
  geom_bar()
```

On this graph, we can see distribution of three possible variants of "קעמ" pronunciation in the dialect corpus: "ק¸", "רמ" and "רעמ".
50 cases of "ק¸", 44 cases of "רעמ" and 5 cases of "רמ".

```{r}
cho %>%
  ggplot(aes(region, fill = variant)) +
  geom_bar()
```

This graph illustrates which regions are included in the dialect corpus and which kinds of "קעמ" pronunciation exist in these regions.
Although the dialect variant "ק¸" is leading in the whole corpus (50 cases), in 5 regions out of 8 the non-dialect variant "רעמ" is predominant.

```{r}
cho %>%
  ggplot(aes(age, variant, colour = gender)) +
  geom_point(size = 2)
```

This graph shows the gender and the age of the informants. 
96% of the informants are female.
The median age is 76.
The maximum age is 96.
The minimum age is 32.

```{r}
cho %>%
  ggplot(aes(birth_year, variant, colour = gender)) +
  geom_point(size = 1)
```

Here is a distribution of our informants according to their age of birth.
The median year of birth is 1924.
The earliest year of birth is 1870.
The latest year of birth is 1979.

```{r}
cho %>%
  ggplot(aes(position, fill = variant)) +
  geom_bar()
```

We also explored which parts of speech "קעמ"-words are in each case.
In the corpus, there are 56 pronouns, 32 conjunctions and 11 particles.
According to the graph, pronouns are mostly pronounced as "ק¸", while conjunctions and particles as "רעמ".

```{r}
cho %>%
  ggplot(aes(variant, fill = yat_hard)) +
  geom_bar()
cho %>%
  ggplot(aes(variant, fill = yat_soft)) +
  geom_bar()
cho %>%
  ggplot(aes(variant, fill = g)) +
  geom_bar()
cho %>%
  ggplot(aes(variant, fill = ts)) +
  geom_bar()
cho %>%
  ggplot(aes(variant, fill = ch)) +
  geom_bar()
```

Having explored the dataset, we found out that there is not much difference concerning other dialect features, as we expected at the beginning. 

We decided not to comment each of these graphs. 
We can only point out that that the biggest difference was found with the last ch feature.

In order to predict one of three possible variants of pronunciation, we decided to apply decision trees.
Firstly, it was necessary to define the importance of our variables.

```{r}
library(party)
cho$region_num <- as.factor(cho$region_num)
set.seed(123) 
train <- 1:68
train.cho <- cho[train,]
test.cho <- cho[-train,]
tr <- cforest(variant~region_num + gender + age + birth_year + position + yat_hard + yat_soft + vocalism + g + ts + ch, data = train.cho,
           controls = cforest_unbiased(ntree=1000, mtry=3))
varimp(tr)
```

According to the results, we choosed the following features for our model: region, age and position.

```{r}
test_answers <- test.cho$variant
tr <- cforest(variant~region_num + age + position, data = train.cho,
           controls = cforest_unbiased(ntree=1000, mtry=3))
predictions <- predict(tr, newdata=test.cho)
results <- test_answers == predictions
results
results_true <- results[results == TRUE]
accuracy  <- length(results_true) / length(test_answers)
accuracy
```

The accuracy of the model was 0.61.

Then we decided to delete all the "רמ" variants from the corpus, in order to make the choice of our model simpler.

```{r}
cho <- read.csv("C:/Users/1/Desktop/cho_dataset.csv", sep = ";")
cho <- cho[-c(34, 35, 36, 86, 87),]
train <- 1:65
train.cho <- cho[train,]
test.cho <- cho[-train,]
test_answers <- test.cho$variant
tr <- cforest(variant~region_num + age + position, data = train.cho,
           controls = cforest_unbiased(ntree=1000, mtry=3))
predictions <- predict(tr, newdata=test.cho)
results <- test_answers == predictions
results
results_true <- results[results == TRUE]
accuracy  <- length(results_true) / length(test_answers)
accuracy
```

The result was better. The accuracy of the model became 0.76.

Finally, we made a decision to also delete several outliers (age).

```{r}
cho <- read.csv("C:/Users/1/Desktop/cho_dataset.csv", sep = ";")
cho <- cho[-c(34, 35, 36, 49, 50, 63, 86, 87),]
train <- 1:62
train.cho <- cho[train,]
test.cho <- cho[-train,]
test_answers <- test.cho$variant
tr <- cforest(variant~region_num + age + position, data = train.cho,
           controls = cforest_unbiased(ntree=1000, mtry=3))
predictions <- predict(tr, newdata=test.cho)
results <- test_answers == predictions
results
results_true <- results[results == TRUE]
accuracy  <- length(results_true) / length(test_answers)
accuracy
```

The accuracy of the model did not change.
So the best accuracy with decision trees was after deleting "רמ", but without deleting outliers.



Apart from CART, we used KNN for classification. Before applying the method, we normalized our data: 

```{r}
library(class)
cho <- read.csv("C:/Users/1/Desktop/cho_dataset.csv", sep = ";")
myvars <- c("variant_num", "region_num", "gender_num", "age", "birth_year", "position", "yat_hard_num", "yat_soft_num", "vocalism_num", "g_num", "ts", "ch_num")
cho.subset <- cho[myvars]
cho_new <- cho.subset
cho$variant_num <- as.factor(cho$variant_num)
cho_new$gender_num <- scale(cho_new$gender_num)
cho_new$age <- scale(cho_new$age)
cho_new$birth_year <- scale(cho_new$birth_year)
cho_new$yat_hard_num <- scale(cho_new$yat_hard_num)
cho_new$yat_soft_num <- scale(cho_new$yat_soft_num)
cho_new$vocalism_num <- scale(cho_new$vocalism_num)
cho_new$g_num <- scale(cho_new$g_num)
cho_new$ch_num <- scale(cho_new$ch_num)
cho_new$region_num <- as.factor(cho_new$region_num)
region <- model.matrix(~region_num + 0, data=cho_new)
region <- scale(region)
cho_new$position <- as.factor(cho_new$position)
position <- model.matrix(~position + 0, data=cho_new)
position <- scale(position)
cho_new$ts <- as.factor(cho_new$ts)
ts <- model.matrix(~ts + 0, data=cho_new)
ts <- scale(ts)
cho_new$region_num <- NULL
cho_new$position <- NULL
cho_new$ts <- NULL
cho_final <- cbind(cho_new, region)
cho_final <- cbind(cho_final, position)
cho_final <- cbind(cho_final, ts)
View(cho_final)
set.seed(123) 
```

We wanted to know not only the accuracy, but also which features are the most important for our model with KNN.

```{r}
train <- 1:68
max <- 0
for (i in 1:511){
  copy_i <- i
  copy_cho <- cho_final
  comb <- vector()
  for (j in 9:2){
    k <- copy_i %% 2
    if (k == 0){
      copy_cho[,j] <- NULL
    } else{
      comb <- c(comb, j)
    }
    copy_i <- copy_i %/% 2
  }
  train.cho <- copy_cho[train,]
  test.cho <- copy_cho[-train,]
  train.var <- copy_cho$variant_num[train]
  knn.2 <-  knn(train.cho, test.cho, train.var, k=2)
  test_answers <- test.cho$variant_num
  results <- test_answers == knn.2
  results_true <- results[results == TRUE]
  accuracy  <- length(results_true) / length(test_answers)
  if (accuracy > max){
    max <- accuracy
    best_comb <- comb
  }
}
print (max)
print (best_comb)

```

The best accuracy was 0.87.
The features were region, position, ts (these three remained in any case), age, yat soft and g.
We were not sure that ts is an important feature, so we deleted it and checked the accuracy.

```{r}
cho_final[, 21:23] <- NULL
train <- 1:68
max <- 0
for (i in 1:511){
  copy_i <- i
  copy_cho <- cho_final
  comb <- vector()
  for (j in 9:2){
    k <- copy_i %% 2
    if (k == 0){
      copy_cho[,j] <- NULL
    } else{
      comb <- c(comb, j)
    }
    copy_i <- copy_i %/% 2
  }
  train.cho <- copy_cho[train,]
  test.cho <- copy_cho[-train,]
  train.var <- copy_cho$variant_num[train]
  knn.2 <-  knn(train.cho, test.cho, train.var, k=2)
  test_answers <- test.cho$variant_num
  results <- test_answers == knn.2
  results_true <- results[results == TRUE]
  accuracy  <- length(results_true) / length(test_answers)
  if (accuracy > max){
    max <- accuracy
    best_comb <- comb
  }
}
print (max)
print (best_comb)
```

The accuracy did not change.
The features were region, position, age and yat_soft.


Then we tried one neighbour with these features.

```{r}
notvars <- c("gender_num", "birth_year", "yat_hard_num", "vocalism_num", "g_num", "ts", "ch_num")
cho_final[notvars] <- NULL
train <- 1:68
train.cho <- cho_final[train,]
test.cho <- cho_final[-train,]
train.var <- cho_final$variant_num[train]
knn.1 <-  knn(train.cho, test.cho, train.var, k=1)
results <- test_answers == knn.1
results
results_true <- results[results == TRUE]
accuracy  <- length(results_true) / length(test_answers)
accuracy
```

The accuracy was 0.87.

Finally, we deleted "רמ"-variants and get our best accuracy with KNN, which was 0.93:
```{r}
cho_final <- cho_final[-c(34, 35, 36, 86, 87),]
train <- 1:65
train.cho <- cho_final[train,]
test.cho <- cho_final[-train,]
train.var <- cho_final$variant_num[train]
test.var <- cho_final$variant_num[-train]
knn.1 <-  knn(train.cho, test.cho, train.var, k=1)
test_answers <- test.cho$variant_num
results <- test_answers == knn.1
results
results_true <- results[results == TRUE]
accuracy  <- length(results_true) / length(test_answers)
accuracy
```

We chose region, position and age to built linear regression model. 
We also included realisation of /ק/ as the most important feature among other dialect features according to the graphs and the results of varimp() with decision trees to check whether it will be important for our model.
We deleted lines with "רמ".

```{r}
cho_1 <- read.csv('C:/Users/1/Desktop/cho_dataset.csv', header = TRUE, sep=";", stringsAsFactors = FALSE)
cho <- cho_1[-c(34, 35, 36, 63, 86, 87),]

cho_dataset <- function(f) {
  cho <- cho
  
  cho$variant <- as.factor(cho$variant)
  cho$ch <- as.factor(cho$ch)
  cho$position <- as.factor(cho$position)
  
  cho <- cho[,c("variant","ch","position","age","region_num")]
  cho <- cho[complete.cases(cho),]
  
  return(cho)
}

cho <- cho_dataset(cho)


str(cho)
```

The model was trained on N-10 cases, while remaining 10 were used for testing.

```{r}
log_reg <- function(df, size=10) {
  N <- nrow(df)
  size=10
  
  df <- df[sample(N),]
  
  num <- floor(N/size)
  rest <- N - num * size
  ncv <- cumsum(c(rep(size,num), rest))
  
  predictions <- data.frame(variant = df$variant, pred = NA)
  
  for(n in ncv) {
    v <- rep(TRUE, N)
    v[(n-size+1):n] <- FALSE
    
    lr <- glm(variant ~ ., data = df[v,], family = binomial(logit))
    predictions[!v,"pred"] <- predict(lr, newdata=df[!v,], type="response")
  }
  
  return(predictions)
}

predictions <- log_reg(cho, size=10)
str(predictions)
```

```{r}
lr <- glm(variant ~ ., data = cho, family = binomial(logit))

summary(lr)
```

```{r}
exp(lr$coefficients)
```

The results say that ch type is not important for logistic regression model (>1).

```{r}
lr_reduced <- glm(variant ~ position + age + region_num , data = cho, family = binomial(logit))

summary(lr_reduced)
```

```{r}
exp(lr_reduced$coefficients)
```

Then we compared two models with and without ch variable.

```{r}
anova(lr, lr_reduced, test = 'Chisq')
```

If we compare two models with and without ch-type factor, we see, that there is no significant difference between them (Pr(>Chi) = 0.5016). 
So we can use a reduced model.
The results mean that ch-type is actualy not important.

```{r}
library('tidyverse')
plot_pred_type_distribution <- function(df, threshold) {
  v <- rep(NA, nrow(df))
  v <- ifelse(df$pred >= threshold & df$variant == 1, "TP", v)
  v <- ifelse(df$pred >= threshold & df$variant == 0, "FP", v)
  v <- ifelse(df$pred < threshold & df$variant == 1, "FN", v)
  v <- ifelse(df$pred < threshold & df$variant == 0, "TN", v)
  
  df$pred_type <- v
  
  ggplot(data=df, aes(x=variant, y=pred)) + 
    geom_violin(fill=rgb(1,1,1,alpha=0.6), color=NA) +
    geom_jitter(aes(color = variant)) +
    geom_hline(yintercept=threshold, color="red", alpha=0.6) +
    scale_color_discrete(name = "type") +
    labs(title=sprintf("Threshold at %.2f", threshold))
}

plot_pred_type_distribution(predictions, 0.6)
```

Let's consider that points closer to 1 - "רעמ" pronunciation. And those closer to 0 - "ק¸".
We desided to put a threshold at 0.6. If we move up a threshold, the number of FP lowers and number of FN increases.
This plot illlustrates distribution of predictions made by the model.



Conclusion:
We tried different models to predict type of pronunciation of "קעמ" in Russian dialects.
The results show that not all of the variables in our dataset are significant for predictions.
For instance, g, ts and vocalism have a very low influence on the קעמ pronunciation.
However, age, region and pos type are important.
That means that our initial hypothesis wasn't confirmed. According to the results, we can say that "קעמ" pronunciation is independent from other pronunciation features described in the dataset.
